{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vWLpRdaOpae"
      },
      "source": [
        "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
        "\n",
        "# LangChain Expression Language (LCEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T_pqwY4Opae"
      },
      "source": [
        "The **L**ang**C**hain **E**xpression **L**anguage (LCEL) is a abstraction of some interesting Python concepts into a format that enables a \"minimalist\" code layer for building chains of LangChain components.\n",
        "\n",
        "LCEL comes with strong support for:\n",
        "\n",
        "1. Superfast development of chains.\n",
        "2. Advanced features such as streaming, async, parallel execution, and more.\n",
        "3. Easy integration with LangSmith and LangServe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y langchain langchain-core langchain-community langchain-cohere"
      ],
      "metadata": {
        "id": "tDy0C_FbnKxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain-core langchain langchain-community langchain-cohere"
      ],
      "metadata": {
        "id": "i2Wwc67ilnA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqYSgGbCOpaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc621aa-26bb-4346-8cc6-d5323746d94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.1/808.1 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.4/265.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    langchain==0.3.1 \\\n",
        "    langchain-openai \\\n",
        "    anthropic==0.7.7 \\\n",
        "    cohere==4.37 \\\n",
        "    docarray==0.39.1\\\n",
        "    langchain_chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMUhsz4kOpag"
      },
      "source": [
        "## LCEL Syntax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1EA7r6MOpag"
      },
      "source": [
        "To understand LCEL syntax let's first build a simple chain in typical Python syntax."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.chat_models import ChatAnthropic\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os"
      ],
      "metadata": {
        "id": "_SS2fy4OiS6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUitrZNdilll",
        "outputId": "d9136595-1a35-44b7-cc40-233e7c0b3e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = open('/content/drive/MyDrive/Generative AI Content/key.txt','r').read()"
      ],
      "metadata": {
        "id": "ckGRwEyuiVHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an instance of ChatOpenAI with the model set to 'gpt-4-turbo'\n",
        "llm = ChatOpenAI(model='gpt-4-turbo')"
      ],
      "metadata": {
        "id": "qWBJ_MLXiafw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOuYyJ2nOpah"
      },
      "outputs": [],
      "source": [
        "# ANTHROPIC_API_KEY = \"<<YOUR_ANTHROPIC_API_KEY>>\"\n",
        "\n",
        "# if ANTHROPIC_API_KEY == \"<<YOUR_ANTHROPIC_API_KEY>>\":\n",
        "#     raise ValueError(\"Please set your ANTHROPIC_API_KEY\")\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Give me small report about {topic}\"\n",
        ")\n",
        "\n",
        "# model = ChatAnthropic(\n",
        "#     model=\"claude-2.1\",\n",
        "#     max_tokens_to_sample=512,\n",
        "#     anthropic_api_key=ANTHROPIC_API_KEY\n",
        "# )  # swap Anthropic for OpenAI with `ChatOpenAI` and `openai_api_key`\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goot38iVOpai",
        "outputId": "0a4656f0-6d00-4691-e9ed-83cd9f69c340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Artificial Intelligence: An Overview\n",
            "\n",
            "**Introduction**  \n",
            "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. This field intersects with computer science and involves the creation and application of algorithms in a dynamic computing environment to perform tasks commonly associated with intelligent beings.\n",
            "\n",
            "**Historical Context**  \n",
            "The concept of AI originated with the ancient Greeks, but the foundation of the modern AI we recognize today started in the mid-20th century. The formal founding of AI as an academic discipline is dated to 1956, during a conference at Dartmouth College, led by John McCarthy who is known as the father of AI.\n",
            "\n",
            "**Key Technologies**  \n",
            "1. **Machine Learning (ML)**: This involves algorithms that allow software to become more accurate in predicting outcomes without being explicitly programmed. It uses statistical methods to enable machines to improve with experience.\n",
            "2. **Natural Language Processing (NLP)**: This involves processing and analyzing large amounts of natural language data to enable computers to understand and interpret human language. Common applications include chatbots and speech recognition systems.\n",
            "3. **Robotics**: This branch of AI involves the design and manufacture of robots that can perform tasks usually requiring human intelligence. These can range from industrial robots to autonomous vehicles.\n",
            "4. **Computer Vision**: This technology enables computers to interpret and make decisions based on visual information from the surrounding environment.\n",
            "\n",
            "**Applications of AI**  \n",
            "AI technology is prevalent across various sectors:\n",
            "- **Healthcare**: AI systems provide diagnostics, personalized medicine, and can manage chronic conditions outside traditional healthcare settings.\n",
            "- **Finance**: AI is used for algorithmic trading, fraud detection, and customer service operations.\n",
            "- **Automotive**: Development of autonomous vehicles and driver-assistance systems.\n",
            "- **Customer Service**: AI chatbots and virtual assistants to enhance customer interaction.\n",
            "\n",
            "**Challenges and Ethical Considerations**  \n",
            "Despite its benefits, AI poses several challenges including job displacement, privacy issues, and the need for large amounts of data which can perpetuate biases. AI ethics is an important area of research focused on ensuring AI technologies are developed and implemented responsibly.\n",
            "\n",
            "**Future of AI**  \n",
            "As hardware improvements continue and research deepens, AI is expected to grow in sophistication, with advancements in quantum computing potentially unlocking new algorithms and capabilities. There's an emphasis on making AI more interpretable, secure, and fair to ensure it aligns with societal norms and expectations.\n",
            "\n",
            "**Conclusion**  \n",
            "AI continues to evolve, promising profound changes in the way we live and work. As the field advances, it brings with it both opportunities and challenges, necessitating careful consideration of its impact on society and individual lives.\n"
          ]
        }
      ],
      "source": [
        "lcel_chain = prompt | llm | output_parser\n",
        "\n",
        "# and run\n",
        "out = lcel_chain.invoke({\"topic\": \"Artificial Intelligence\"})\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yEy2mAvOpaj"
      },
      "source": [
        "Pretty cool, the way that this pipe operator is being used is that it is taking output from the function to the _left_ and feeding it into the function on the _right_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2fCMl7nOpaj"
      },
      "source": [
        "## How the Pipe Operator Works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2CyiG3OOpaj"
      },
      "source": [
        "To really understand LCEL we can take a look at how this pipe operation works. We know it takes output from the _right_ and feeds it to the _left_ — but this isn't typical Python, so how is this implemented? Let's try creating our own version of this with some simple functions.\n",
        "\n",
        "We will be using the `__or__` method within Python class objects. When we place two classes together like `chain = class_a | class_b` the Python interpreter will check if these classes contain this `__or__` method. If they do then our `|` code will be translated into `chain = class_a.__or__(class_b)`.\n",
        "\n",
        "That means both of these patterns will return the same thing:\n",
        "\n",
        "```python\n",
        "# object approach\n",
        "chain = class_a.__or__(class_b)\n",
        "chain(\"some input\")\n",
        "\n",
        "# pipe approach\n",
        "chain = class_a | class_b\n",
        "chain(\"some input\")\n",
        "\n",
        "```\n",
        "\n",
        "With that in mind, we can build a `Runnable` class that consumes a function and turns it into a function that can be chained with other functions using the pipe operator `|`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j1Y84vZOpaj"
      },
      "outputs": [],
      "source": [
        "class Runnable:\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "\n",
        "    def __or__(self, other):\n",
        "        def chained_func(*args, **kwargs):\n",
        "            # the other func consumes the result of this func\n",
        "            return other(self.func(*args, **kwargs))\n",
        "        return Runnable(chained_func)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.func(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HonZG6awOpaj"
      },
      "source": [
        "Let's implement this to take the value `3`, add `5` (giving `8`), and multiply by `2` (giving `16`).\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwnL31LUOpaj",
        "outputId": "5fed3749-bc84-431e-a89b-bcdbe0a67e69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def add_five(x):\n",
        "    return x + 5\n",
        "\n",
        "def multiply_by_two(x):\n",
        "    return x * 2\n",
        "\n",
        "# wrap the functions with Runnable\n",
        "add_five = Runnable(add_five)\n",
        "multiply_by_two = Runnable(multiply_by_two)\n",
        "\n",
        "# run them using the object approach\n",
        "chain = add_five.__or__(multiply_by_two)\n",
        "chain(3)  # should return 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3Xq3jvZOpaj"
      },
      "source": [
        "Using `__or__` directly we get the correct answer, now let's try using the pipe operator `|` to chain them together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zYUql8MOpaj",
        "outputId": "72c0708a-adbe-4198-d75a-22d244d8305a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# chain the runnable functions together\n",
        "chain = add_five | multiply_by_two\n",
        "\n",
        "# invoke the chain\n",
        "chain(3)  # we should return 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmq28-RAOpak"
      },
      "source": [
        "Using either method we get the same response, at it's core this is what LCEL is doing, but there is more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_1r31-eOpak"
      },
      "source": [
        "## LCEL Deep Dive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amNI66IUOpak"
      },
      "source": [
        "Now that we understand what this syntax is doing under the hood, let's explore it within the context of LCEL and see a few of the additional methods that LangChain has provided to maximize flexibility when working with LCEL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G0ZnFVFOpak"
      },
      "source": [
        "### Runnables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgkoDXxROpak"
      },
      "source": [
        "When working with LCEL we may find that we need to modify the structure or values being passed between components — for this we can use _runnables_. Let's try:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain_community\n",
        "! pip install -U langchain-cohere"
      ],
      "metadata": {
        "id": "6UlIC3SIjO7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aaa9526-be96-4a03-e0d3-5f30c034600e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.35)\n",
            "Collecting langchain<1.0.0,>=0.3.18 (from langchain_community)\n",
            "  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.1.147)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain_community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain_community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.18-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: marshmallow, httpx-sse, dataclasses-json, pydantic-settings, langchain, langchain_community\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.1\n",
            "    Uninstalling langchain-0.3.1:\n",
            "      Successfully uninstalled langchain-0.3.1\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.18 langchain_community-0.3.17 marshmallow-3.26.1 pydantic-settings-2.7.1\n",
            "Collecting langchain-cohere\n",
            "  Downloading langchain_cohere-0.4.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting cohere<6.0,>=5.12.0 (from langchain-cohere)\n",
            "  Downloading cohere-5.13.12-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (0.3.17)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (0.3.35)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (2.10.6)\n",
            "Collecting types-pyyaml<7.0.0.0,>=6.0.12.20240917 (from langchain-cohere)\n",
            "  Downloading types_PyYAML-6.0.12.20241230-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (0.21.0)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (2.32.0.20241016)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (4.12.2)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.18 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.18)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.0.38)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.7.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.26.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-cohere) (0.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain-cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain-cohere) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain-cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain-cohere) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain-cohere) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-cohere) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.6)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.12.0->langchain-cohere) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.12.0->langchain-cohere) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain-cohere) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain-cohere) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain-cohere) (2024.10.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain-cohere) (4.67.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain-cohere) (1.3.1)\n",
            "Downloading langchain_cohere-0.4.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.13.12-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_PyYAML-6.0.12.20241230-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-pyyaml, cohere, langchain-cohere\n",
            "  Attempting uninstall: cohere\n",
            "    Found existing installation: cohere 4.37\n",
            "    Uninstalling cohere-4.37:\n",
            "      Successfully uninstalled cohere-4.37\n",
            "Successfully installed cohere-5.13.12 langchain-cohere-0.4.2 types-pyyaml-6.0.12.20241230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1oZvXmdOpak"
      },
      "outputs": [],
      "source": [
        "# from langchain.embeddings import CohereEmbeddings\n",
        "# from langchain.vectorstores import DocArrayInMemorySearch\n",
        "import cohere\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "\n",
        "api_key = '1XyHUHdLz726y08PVVSg3YQ7UD3BsRq8SxYpPjG9'\n",
        "\n",
        "\n",
        "# embedding = CohereEmbeddings(\n",
        "#     model=\"embed-english-light-v3.0\",\n",
        "#     cohere_api_key=api_key\n",
        "# )\n",
        "\n",
        "import langchain_cohere\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "\n",
        "embedding = CohereEmbeddings(\n",
        "    model=\"embed-english-light-v3.0\",\n",
        "    cohere_api_key=api_key\n",
        ")\n",
        "\n",
        "# vecstore_a = DocArrayInMemorySearch.from_texts(\n",
        "#     [\"half the info will be here\", \"James' birthday is the 7th December\"],\n",
        "#     embedding=embedding,\n",
        "\n",
        "# )\n",
        "# vecstore_b = DocArrayInMemorySearch.from_texts(\n",
        "#     [\"and half here\", \"James was born in 1994\"],\n",
        "#     embedding=embedding\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs_a = [Document(page_content=doc) for doc in [\"half the info will be here\", \"James' birthday is the 7th December\"]]\n",
        "docs_b = [Document(page_content=doc) for doc in [\"and half here\", \"James was born in 1994\"]]"
      ],
      "metadata": {
        "id": "S-bhtfBPFQo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_db_a = Chroma.from_documents(documents=docs_a,\n",
        "                                  embedding=embedding)\n",
        "chroma_db_b = Chroma.from_documents(documents=docs_b, collection_name='wiki_db',\n",
        "                                  embedding=embedding)"
      ],
      "metadata": {
        "id": "rKFQkmNxGhMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxj5zBhwOpak"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "First let's try passing a question through to a single one of these `vecstore` objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-__lOQHOpak"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import (\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough\n",
        ")\n",
        "\n",
        "retriever_a = chroma_db_a.as_retriever(search_kwargs={\"k\": min(4, len(docs_a))})\n",
        "retriever_b = chroma_db_b.as_retriever(search_kwargs={\"k\": min(4, len(docs_a))})\n",
        "\n",
        "prompt_str = \"\"\"Answer the question below using the context:\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
        "\n",
        "retrieval = RunnableParallel(\n",
        "    {\"context\": retriever_a, \"question\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "chain = retrieval | prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhGBZk9MOpak",
        "outputId": "d466efab-36db-41b0-ecba-fe5ee14e0390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "James' birthday is on the 7th December.\n"
          ]
        }
      ],
      "source": [
        "out = chain.invoke(\"when was James born?\")\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8PAtL7wOpak"
      },
      "source": [
        "Here we have used `RunnableParallel` to create two parallel streams of information, one for `\"context\"` that is information fed in from `retriever_a`, and another for `\"question\"` which is the _passthrough_ information, ie the information that is passed through from our `chain.invoke(\"when was James born?\")` call.\n",
        "\n",
        "Using this information the chain is close to answering the question but it doesn't have enough information, it is missing the information that we have stored in `retriever_b`. Fortunately, we can have multiple parallel information streams using the `RunnableParallel` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-XPoNdOOpak"
      },
      "outputs": [],
      "source": [
        "prompt_str = \"\"\"Answer the question below using the context:\n",
        "\n",
        "Context:\n",
        "{context_a}\n",
        "{context_b}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
        "\n",
        "retrieval = RunnableParallel(\n",
        "    {\n",
        "        \"context_a\": retriever_a, \"context_b\": retriever_b,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        ")\n",
        "\n",
        "chain = retrieval | prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3-a5RzcOpal",
        "outputId": "e361c5c3-42b5-4883-ebb3-f84e03ff3443",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "James was born on 7th December 1994.\n"
          ]
        }
      ],
      "source": [
        "out = chain.invoke(\"when was James born?\")\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvALpW42Opal"
      },
      "source": [
        "Now the `RunnableParallel` object is allowing us to search with `retriever_a` _and_ `retriever_b` in parallel, ie at the same time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob3VwSBoOpal"
      },
      "source": [
        "## Runnable Lambdas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1mX6O8DOpal"
      },
      "source": [
        "The `RunnableLambda` is a LangChain abstraction that allows us to turn Python functions into pipe-compatible function, similar to the `Runnable` class we created near the beginning of this notebook.\n",
        "\n",
        "Let's try it out with our earlier `add_five` and `multiply_by_two` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYoU546BOpal"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# wrap the functions with RunnableLambda\n",
        "add_five = RunnableLambda(add_five)\n",
        "multiply_by_two = RunnableLambda(multiply_by_two)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWgt3S5FOpal"
      },
      "source": [
        "As with our earlier `Runnable` abstraction, we can use `|` operators to chain together our `RunnableLambda` abstractions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOtV2y-kOpal"
      },
      "outputs": [],
      "source": [
        "chain = add_five | multiply_by_two"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK6SJu_6Opal"
      },
      "source": [
        "Unlike our `Runnable` abstraction, we cannot run the `RunnableLambda` chain by calling it directly, instead we must call `chain.invoke`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIpVp3KuOpal",
        "outputId": "3b48e92b-e252-4433-90a2-2914f97c9a2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxh8UG4-Opaq"
      },
      "source": [
        "As before, we can see the same answer. Naturally we can feed custom functions into our chains using this approach. Let's try a short chain and see where we might want to insert a custom function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxymN22TOpaq"
      },
      "outputs": [],
      "source": [
        "prompt_str = \"Tell me an short fact about {topic}\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
        "\n",
        "chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIKpP3wjOpaq",
        "outputId": "19a58e64-4aa7-4bdc-f102-234ab6f1a463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Artificial Intelligence (AI) has the capability to learn and adapt over time. Techniques such as machine learning enable AI systems to improve their performance on tasks by analyzing large amounts of data and identifying patterns, without being explicitly programmed for each specific task.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNslrAiCOpar",
        "outputId": "e3f917f3-edf3-4734-f84b-a26c648166c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Artificial Intelligence (AI) can learn and make decisions independently. AI systems are trained using large amounts of data, allowing them to recognize patterns and perform tasks such as translating languages, recommending products, and driving autonomous vehicles.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjXl_iwpOpar"
      },
      "source": [
        "The returned text always includes the initial `\"Here's a short fact about ...\\n\\n\"` — let's add a function to split on double newlines `\"\\n\\n\"` and only return the fact itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlo1tLmbOpar"
      },
      "outputs": [],
      "source": [
        "def extract_fact(x):\n",
        "    if \"\\n\\n\" in x:\n",
        "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "get_fact = RunnableLambda(extract_fact)\n",
        "\n",
        "chain = prompt | llm | output_parser | get_fact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dshBQYW6Opar",
        "outputId": "968e74ac-6741-4b1f-b9ab-40a988035d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Artificial intelligence (AI) can be categorized into two main types: narrow AI, which is designed to perform a narrow task (like facial recognition or internet searches), and general AI, which can understand and reason its environment as a human would. Most existing AI applications, such as voice assistants and recommendation systems, are forms of narrow AI.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK2yDsXCOpar",
        "outputId": "d0bfe5b3-c1dd-4ee7-9f6d-8df41cd644d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Artificial Intelligence (AI) is the branch of computer science that aims to create machines capable of performing tasks that would normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI can be categorized into two main types: narrow AI, which is designed to perform specific tasks, and general AI, which aims to perform any intellectual task that a human can do.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtEsQGWZOpar"
      },
      "source": [
        "Now we're getting well formatted outputs using our final custom `get_fact` function.\n",
        "\n",
        "---\n",
        "\n",
        "That covers the essentials you need to getting started and building with the **L**ang**C**hain **E**xpression **L**anguage (LCEL). With it, we can put together chains very easily — and the current focus of the LangChain team is on further LCEL development and support.\n",
        "\n",
        "The pros and cons of LCEL are varied. Those that love it tend to focus on the minimalist code style, LCEL's support for streaming, parallel operations, and async, and LCEL's nice integration with LangChain's focus on chaining components together.\n",
        "\n",
        "There are also people that are less fond of LCEL. Typically people will point to it being yet another abstraction on top of an already very abstract library, that the syntax is confusing, against [the Zen of Python](https://peps.python.org/pep-0020/), and requires too much effort to learn new (or uncommon) syntax.\n",
        "\n",
        "Both viewpoints are entirely valid, LCEL is a very different approach — ofcourse there are major pros and major cons. In either case, if you're willing to spend some time learning the syntax, it allows us to develop very quickly, and with that in mind it's well worth learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmvxPribOpar"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}